{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6984f532-b7ea-4e56-83a4-2271f4d59c42",
   "metadata": {},
   "source": [
    "## Plot: A company launches an AI chat agent through a REST API that is smart in writing code. \n",
    "  \n",
    "### Users seem to love it and use it consistently. The company starts to be profitable as their API has many paid users, but there are these concerns:\n",
    "- Users compare to other services and find it consistently slow, especially for trivial cases.\n",
    "- The company finds out that profitability is starting to look quite low in comparison with competitors.\n",
    "  \n",
    "**Diagnosis**: With some analysis, the company finds out that the model they are using is quite accurate but at the expense of speed and high price. They look at the queries that the users are bringing are only 50% code related, the rest of them are trivial questions that are factual.\n",
    "  \n",
    "**Solution**: Company comes to the conclusion that approximately half the queries can be answered faster model and without dropping accuracy with a weaker model. They have a chance to improve speed to contribute to the UX of many queries while only using longer wait times and more resources (infrastructure costs) for code related queries. To solve this, the company chooses to utilise a router that routes factual and general queries to a weaker model and code related queries to a strong model. For the sake of analogies let the weaker model be the Llama 3.1 8B model and the stronger model be Qwen3:coder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f70285-86b2-4157-a60e-b65ee85c63ec",
   "metadata": {},
   "source": [
    "Generally routing can be done based on the properties of the prompt/query. Some possible properties can be extracted semantically (with use of an embedder) which is what is followed in part 1 (below). The property of a prompt can also be extracted as per what category of the content the prompt falls in, with the use of another LLM, which is what is followed in part 2 (further below). But also, as per the target use of the router, a prompt property can be learned by training a model, which is what is done in part 3 (router_training_notebook.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387349fb-5553-472e-804c-b6a2e659e0dc",
   "metadata": {},
   "source": [
    "**Datasets** used can be understood in the data_notebook.ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ce2c2-0e78-4ba8-bc93-a634e28e76ba",
   "metadata": {},
   "source": [
    "# Part One\n",
    "\n",
    "### There are multiple ways of making a router, we try three way. The first one is where we encode every query of the user and try to match with the personalities our available models. The following is the code for this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1058de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, Literal, Callable, Optional\n",
    "from collections import defaultdict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f84ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load the dataset from either JSONL or CSV format.\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if file_path.suffix == '.jsonl':\n",
    "        data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line.strip()))\n",
    "        return data\n",
    "    \n",
    "    elif file_path.suffix == '.csv':\n",
    "        data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                data.append(row)\n",
    "        return data\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_path.suffix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b94562ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# State definition for the graph\n",
    "class GraphState(TypedDict):\n",
    "    prompt: str\n",
    "    response: str\n",
    "    agent_type: str\n",
    "    predicted_category: str\n",
    "    actual_category: str\n",
    "\n",
    "def embed(text, embedder) -> np.ndarray:\n",
    "    \"\"\"Embed text using sentence-transformers.\"\"\"\n",
    "    return embedder.encode(text, convert_to_numpy=True)\n",
    "\n",
    "def make_router_node(\n",
    "    embedder: SentenceTransformer,\n",
    "    coding_emb: np.ndarray,\n",
    "    chat_emb: np.ndarray,\n",
    ") -> Callable[[GraphState], GraphState]:\n",
    "    def router_node(state: GraphState) -> GraphState:\n",
    "        prompt = state[\"prompt\"]\n",
    "        user_emb = embed(prompt, embedder)\n",
    "\n",
    "        sim_coding = cosine_similarity(user_emb, coding_emb)\n",
    "        sim_chat = cosine_similarity(user_emb, chat_emb)\n",
    "\n",
    "        if sim_coding > sim_chat:\n",
    "            state[\"agent_type\"] = \"coding\"\n",
    "            state[\"predicted_category\"] = \"coding\"\n",
    "        else:\n",
    "            state[\"agent_type\"] = \"chat\"\n",
    "            state[\"predicted_category\"] = \"factual\"\n",
    "\n",
    "        return state\n",
    "    return router_node\n",
    "\n",
    "# Placeholder coding agent - just counts routing decisions\n",
    "def coding_agent_node(state: GraphState) -> GraphState:\n",
    "    state[\"response\"] = \"[RESPONSE FROM QWEN3:CODER]\"\n",
    "    return state\n",
    "\n",
    "# Placeholder chat agent - just counts routing decisions\n",
    "def chat_agent_node(state: GraphState) -> GraphState:\n",
    "    state[\"response\"] = \"[RESPONSE FROM CHEAP-LLAMA-MODEL]\"\n",
    "    return state\n",
    "\n",
    "# Conditional edge function to determine routing\n",
    "def route_decision(state: GraphState) -> Literal[\"coding_agent\", \"chat_agent\"]:\n",
    "    return \"coding_agent\" if state[\"agent_type\"] == \"coding\" else \"chat_agent\"\n",
    "\n",
    "# Create and configure the graph\n",
    "def create_routing_graph(embedder: SentenceTransformer, \n",
    "                         coding_emb: np.ndarray, \n",
    "                         chat_emb: np.ndarray) -> StateGraph:\n",
    "    workflow = StateGraph(GraphState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"router\", make_router_node(embedder, coding_emb, chat_emb))\n",
    "    workflow.add_node(\"coding_agent\", coding_agent_node)\n",
    "    workflow.add_node(\"chat_agent\", chat_agent_node)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"router\")\n",
    "    \n",
    "    # Add conditional edges\n",
    "    workflow.add_conditional_edges(\n",
    "        \"router\",\n",
    "        route_decision,\n",
    "        {\n",
    "            \"coding_agent\": \"coding_agent\",\n",
    "            \"chat_agent\": \"chat_agent\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add edges to end\n",
    "    workflow.add_edge(\"coding_agent\", END)\n",
    "    workflow.add_edge(\"chat_agent\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    return workflow.compile()\n",
    "\n",
    "# dataset_files = ['prompts_2000.jsonl']#, 'prompts_2000.csv']\n",
    "def evaluate_routing_accuracy(dataset_file: str,\n",
    "                              embedder: SentenceTransformer,\n",
    "                              coding_emb: np.ndarray,\n",
    "                              chat_emb: np.ndarray):\n",
    "    \"\"\"Evaluate the routing accuracy using the dataset.\"\"\"\n",
    "    \n",
    "    # Try to load the dataset\n",
    "    data = None\n",
    "    \n",
    "    if Path(dataset_file).exists():\n",
    "        print(f\"Loading dataset from {dataset_file}...\")\n",
    "        data = load_dataset(dataset_file)\n",
    "\n",
    "    if data is None:\n",
    "        print(\"Error: No dataset found. Please run dataset_create.py first to generate the dataset.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loaded {len(data)} prompts from dataset\")\n",
    "    \n",
    "    # Create the routing graph\n",
    "    app = create_routing_graph(embedder=embedder,\n",
    "                               coding_emb=coding_emb,\n",
    "                               chat_emb=chat_emb)\n",
    "\n",
    "    # Statistics tracking\n",
    "    stats = defaultdict(int)\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Detailed results\n",
    "    results = []\n",
    "    \n",
    "    print(\"Evaluating routing decisions...\")\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        if i % 200 == 0:\n",
    "            print(f\"Processed {i}/{len(data)} prompts...\")\n",
    "        \n",
    "        prompt = item['prompt']\n",
    "        actual_category = item['category']  # 'coding' or 'factual'\n",
    "        \n",
    "        # Initial state\n",
    "        initial_state = {\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": \"\",\n",
    "            \"agent_type\": \"\",\n",
    "            \"predicted_category\": \"\",\n",
    "            \"actual_category\": actual_category\n",
    "        }\n",
    "        \n",
    "        # Run the graph\n",
    "        result = app.invoke(initial_state)\n",
    "        \n",
    "        predicted_category = result['predicted_category']\n",
    "        \n",
    "        # Count statistics\n",
    "        stats[f\"actual_{actual_category}\"] += 1\n",
    "        stats[f\"predicted_{predicted_category}\"] += 1\n",
    "        \n",
    "        # Check accuracy\n",
    "        is_correct = predicted_category == actual_category\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "            stats[f\"correct_{actual_category}\"] += 1\n",
    "        else:\n",
    "            stats[f\"incorrect_{actual_category}\"] += 1\n",
    "        \n",
    "        total_predictions += 1\n",
    "        \n",
    "        # Store detailed result\n",
    "        results.append({\n",
    "            'id': item.get('id', f'item_{i}'),\n",
    "            'prompt': prompt[:100] + '...' if len(prompt) > 100 else prompt,\n",
    "            'actual': actual_category,\n",
    "            'predicted': predicted_category,\n",
    "            'correct': is_correct,\n",
    "            'agent_routed_to': result['agent_type']\n",
    "        })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    overall_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    \n",
    "    # Calculate per-category metrics\n",
    "    coding_correct = stats['correct_coding']\n",
    "    coding_total = stats['actual_coding']\n",
    "    coding_accuracy = coding_correct / coding_total if coding_total > 0 else 0\n",
    "    \n",
    "    factual_correct = stats['correct_factual']\n",
    "    factual_total = stats['actual_factual']\n",
    "    factual_accuracy = factual_correct / factual_total if factual_total > 0 else 0\n",
    "    \n",
    "    # Confusion matrix\n",
    "    coding_predicted_as_coding = sum(1 for r in results if r['actual'] == 'coding' and r['predicted'] == 'coding')\n",
    "    coding_predicted_as_factual = sum(1 for r in results if r['actual'] == 'coding' and r['predicted'] == 'factual')\n",
    "    factual_predicted_as_coding = sum(1 for r in results if r['actual'] == 'factual' and r['predicted'] == 'coding')\n",
    "    factual_predicted_as_factual = sum(1 for r in results if r['actual'] == 'factual' and r['predicted'] == 'factual')\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ROUTING ACCURACY EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nOverall Accuracy: {overall_accuracy:.3f} ({correct_predictions}/{total_predictions})\")\n",
    "    \n",
    "    print(f\"\\nPer-Category Accuracy:\")\n",
    "    print(f\"  Coding: {coding_accuracy:.3f} ({coding_correct}/{coding_total})\")\n",
    "    print(f\"  Factual: {factual_accuracy:.3f} ({factual_correct}/{factual_total})\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                    Predicted\")\n",
    "    print(f\"Actual      Coding    Factual\")\n",
    "    print(f\"Coding      {coding_predicted_as_coding:6d}    {coding_predicted_as_factual:7d}\")\n",
    "    print(f\"Factual     {factual_predicted_as_coding:6d}    {factual_predicted_as_factual:7d}\")\n",
    "    \n",
    "    print(f\"\\nRouting Statistics:\")\n",
    "    print(f\"  Total prompts routed to coding agent: {stats['predicted_coding']}\")\n",
    "    print(f\"  Total prompts routed to chat agent: {stats['predicted_factual']}\")\n",
    "    \n",
    "    # Show some examples of misclassifications\n",
    "    print(f\"\\nSample Misclassifications (first 5):\")\n",
    "    misclassified = [r for r in results if not r['correct']][:5]\n",
    "    for i, r in enumerate(misclassified, 1):\n",
    "        print(f\"  {i}. Actual: {r['actual']}, Predicted: {r['predicted']}\")\n",
    "        print(f\"     Prompt: {r['prompt']}\")\n",
    "        print()\n",
    "    \n",
    "    # Save detailed results\n",
    "    output_file = \"routing_evaluation_results.csv\"\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        fieldnames = ['id', 'prompt', 'actual', 'predicted', 'correct', 'agent_routed_to']\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    print(f\"Detailed results saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028819cd-13ad-416d-a732-9698785b0789",
   "metadata": {},
   "source": [
    "### Here we try to use a simple and fast embedder with very basic personalities that can match to the user query. \n",
    "\n",
    "We use LangGraph to form a pipeline for every user query. The query is embedded and a consine distance is calculated against the emmbedded personalities of the models, which ever one comes to be the closest is where the query is routed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e851e-91a2-47a9-be0e-4f0be40581aa",
   "metadata": {},
   "source": [
    "This approach is fast (based on the selected model) and has some levers that can be adjusted in accordance to what the target is (speed, quality, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda12d55-da38-440a-8991-e4e30b492813",
   "metadata": {},
   "source": [
    "We test our approach against a well balanced dataset with 1000 factual queries and 1000 code related queries. For more on dataset generation, look at the data_notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2305a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ./data/prompts_2000.jsonl...\n",
      "Loaded 2000 prompts from dataset\n",
      "Evaluating routing decisions...\n",
      "Processed 0/2000 prompts...\n",
      "Processed 200/2000 prompts...\n",
      "Processed 400/2000 prompts...\n",
      "Processed 600/2000 prompts...\n",
      "Processed 800/2000 prompts...\n",
      "Processed 1000/2000 prompts...\n",
      "Processed 1200/2000 prompts...\n",
      "Processed 1400/2000 prompts...\n",
      "Processed 1600/2000 prompts...\n",
      "Processed 1800/2000 prompts...\n",
      "\n",
      "============================================================\n",
      "ROUTING ACCURACY EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.683 (1367/2000)\n",
      "\n",
      "Per-Category Accuracy:\n",
      "  Coding: 0.820 (820/1000)\n",
      "  Factual: 0.547 (547/1000)\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted\n",
      "Actual      Coding    Factual\n",
      "Coding         820        180\n",
      "Factual        453        547\n",
      "\n",
      "Routing Statistics:\n",
      "  Total prompts routed to coding agent: 1273\n",
      "  Total prompts routed to chat agent: 727\n",
      "\n",
      "Sample Misclassifications (first 5):\n",
      "  1. Actual: factual, Predicted: coding\n",
      "     Prompt: What is the main difference between ORC and Parquet?\n",
      "\n",
      "  2. Actual: factual, Predicted: coding\n",
      "     Prompt: What is Crohn's Disease and what are some of the symptoms?\n",
      "\n",
      "  3. Actual: coding, Predicted: factual\n",
      "     Prompt: Create an HTML table containing 3 row and 3 column of text elements.\n",
      "\n",
      "  4. Actual: coding, Predicted: factual\n",
      "     Prompt: Code a solution that prints out the string \"Hello, [name]!\" using string interpolation in JavaScript...\n",
      "\n",
      "  5. Actual: factual, Predicted: coding\n",
      "     Prompt: Who directed the most episodes of season five of Game of Thrones?\n",
      "\n",
      "Detailed results saved to: routing_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Agent prompts for routing\n",
    "CODING_AGENT_PROMPT = \"I am a coding assistant. I help with programming questions, algorithms, debugging, and software development.\"\n",
    "CHAT_AGENT_PROMPT = \"I am a general chat assistant. I help with everyday questions, facts, explanations, and general knowledge.\"\n",
    "\n",
    "# Embedding model - using sentence-transformers for local embeddings\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and fast model\n",
    "coding_emb = embed(CODING_AGENT_PROMPT, embedder)\n",
    "chat_emb = embed(CHAT_AGENT_PROMPT, embedder)\n",
    "\n",
    "# Evaluate routing accuracy\n",
    "evaluate_routing_accuracy(\"./data/prompts_2000.jsonl\", embedder, coding_emb, chat_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb3b724-c1b6-460f-8ba2-afef60e2f38e",
   "metadata": {},
   "source": [
    "### The current model's low accuracy (68%) is understandbale due to our use of a fast and generic embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971a00a",
   "metadata": {},
   "source": [
    "First misclassication is a tough one but also understandable as it is factual question about programming, so it may fall on both sides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f864a5",
   "metadata": {},
   "source": [
    "At its core this is an embedding classification task, so lets look at the MTEB and choose one of the top performing embedding models over the task of classification specifically.  \n",
    "Looking at the open-source option sorted by classification scores the Qwen3 embedding model falls in the top scoring models.  \n",
    "Lets test it out with our approach.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1bcfb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Fetching 2 files: 100%|██████████| 2/2 [04:25<00:00, 132.83s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.97s/it]\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ./data/prompts_2000.jsonl...\n",
      "Loaded 2000 prompts from dataset\n",
      "Evaluating routing decisions...\n",
      "Processed 0/2000 prompts...\n",
      "Processed 200/2000 prompts...\n",
      "Processed 400/2000 prompts...\n",
      "Processed 600/2000 prompts...\n",
      "Processed 800/2000 prompts...\n",
      "Processed 1000/2000 prompts...\n",
      "Processed 1200/2000 prompts...\n",
      "Processed 1400/2000 prompts...\n",
      "Processed 1600/2000 prompts...\n",
      "Processed 1800/2000 prompts...\n",
      "\n",
      "============================================================\n",
      "ROUTING ACCURACY EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.897 (1794/2000)\n",
      "\n",
      "Per-Category Accuracy:\n",
      "  Coding: 0.931 (931/1000)\n",
      "  Factual: 0.863 (863/1000)\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted\n",
      "Actual      Coding    Factual\n",
      "Coding         931         69\n",
      "Factual        137        863\n",
      "\n",
      "Routing Statistics:\n",
      "  Total prompts routed to coding agent: 1068\n",
      "  Total prompts routed to chat agent: 932\n",
      "\n",
      "Sample Misclassifications (first 5):\n",
      "  1. Actual: factual, Predicted: coding\n",
      "     Prompt: What is the main difference between ORC and Parquet?\n",
      "\n",
      "  2. Actual: factual, Predicted: coding\n",
      "     Prompt: What is Apache Hive?\n",
      "\n",
      "  3. Actual: factual, Predicted: coding\n",
      "     Prompt: What is the difference between a sedan and a coupe?\n",
      "\n",
      "  4. Actual: factual, Predicted: coding\n",
      "     Prompt: Who directed the most episodes of season five of Game of Thrones?\n",
      "\n",
      "  5. Actual: factual, Predicted: coding\n",
      "     Prompt: In Digital Marketing, what is the difference between SEO and SEM?\n",
      "\n",
      "Detailed results saved to: routing_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Agent prompts for routing\n",
    "CODING_AGENT_PROMPT = \"I am a coding assistant. I help with programming questions, algorithms, debugging, and software development.\"\n",
    "CHAT_AGENT_PROMPT = \"I am a general chat assistant. I help with everyday questions, facts, explanations, and general knowledge.\"\n",
    "\n",
    "# Embedding model - using sentence-transformers for local embeddings\n",
    "embedder = SentenceTransformer(\"Qwen/Qwen3-Embedding-4B\")\n",
    "\n",
    "coding_emb = embed(CODING_AGENT_PROMPT, embedder)\n",
    "chat_emb = embed(CHAT_AGENT_PROMPT, embedder)\n",
    "\n",
    "# Evaluate routing accuracy\n",
    "evaluate_routing_accuracy(\"./data/prompts_2000.jsonl\", embedder, coding_emb, chat_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3391dac",
   "metadata": {},
   "source": [
    "### This score is a clear improvement but to generalise to data outside of current distribution it would be smarter to have stronger and wider prompts which capture more words from incoming queries. Let's expand on the personalities of our agent prompts for that. This can help set better guardrails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3be7499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ./data/prompts_2000.jsonl...\n",
      "Loaded 2000 prompts from dataset\n",
      "Evaluating routing decisions...\n",
      "Processed 0/2000 prompts...\n",
      "Processed 200/2000 prompts...\n",
      "Processed 400/2000 prompts...\n",
      "Processed 600/2000 prompts...\n",
      "Processed 800/2000 prompts...\n",
      "Processed 1000/2000 prompts...\n",
      "Processed 1200/2000 prompts...\n",
      "Processed 1400/2000 prompts...\n",
      "Processed 1600/2000 prompts...\n",
      "Processed 1800/2000 prompts...\n",
      "\n",
      "============================================================\n",
      "ROUTING ACCURACY EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.936 (1873/2000)\n",
      "\n",
      "Per-Category Accuracy:\n",
      "  Coding: 0.942 (942/1000)\n",
      "  Factual: 0.931 (931/1000)\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted\n",
      "Actual      Coding    Factual\n",
      "Coding         942         58\n",
      "Factual         69        931\n",
      "\n",
      "Routing Statistics:\n",
      "  Total prompts routed to coding agent: 1011\n",
      "  Total prompts routed to chat agent: 989\n",
      "\n",
      "Sample Misclassifications (first 5):\n",
      "  1. Actual: factual, Predicted: coding\n",
      "     Prompt: What is the main difference between ORC and Parquet?\n",
      "\n",
      "  2. Actual: factual, Predicted: coding\n",
      "     Prompt: What are examples of famous plays written by William Shakespeare?\n",
      "\n",
      "  3. Actual: factual, Predicted: coding\n",
      "     Prompt: In Digital Marketing, what is the difference between SEO and SEM?\n",
      "\n",
      "  4. Actual: factual, Predicted: coding\n",
      "     Prompt: What is the Cassandra database?\n",
      "\n",
      "  5. Actual: factual, Predicted: coding\n",
      "     Prompt: How many episodes in season one of Game of Thrones did Tim Van Patten direct?\n",
      "\n",
      "Detailed results saved to: routing_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Agent prompts for routing - Enhanced with detailed descriptions\n",
    "CODING_AGENT_PROMPT = \"\"\"I am a specialized coding and programming assistant. I help with:\n",
    "- Writing, debugging, and optimizing code in Python, JavaScript, Java, C++, Go, Rust, and other languages\n",
    "- Algorithm design, data structures, and computational complexity analysis\n",
    "- Software architecture, design patterns, and best practices\n",
    "- Web development (HTML, CSS, React, Node.js, APIs, databases)\n",
    "- Machine learning and data science (pandas, numpy, scikit-learn, TensorFlow, PyTorch)\n",
    "- DevOps, CI/CD, Docker, Kubernetes, and cloud deployment\n",
    "- Code reviews, refactoring, and performance optimization\n",
    "- Programming concepts like functions, classes, loops, recursion, and OOP\n",
    "- Framework-specific help (Django, Flask, FastAPI, Spring, Express)\n",
    "- Testing, unit tests, integration tests, and TDD\n",
    "- Version control with Git and collaboration workflows\n",
    "- Technical problem solving and computational thinking\n",
    "- Code documentation, commenting, and maintainability\n",
    "- Package management, dependencies, and build systems\"\"\"\n",
    "\n",
    "CHAT_AGENT_PROMPT = \"\"\"I am a general knowledge and conversational assistant. I help with:\n",
    "- Answering factual questions about history, science, geography, and current events\n",
    "- Explaining concepts in physics, chemistry, biology, mathematics, and other academic subjects\n",
    "- Providing information about culture, arts, literature, and entertainment\n",
    "- Discussing philosophy, ethics, psychology, and social sciences\n",
    "- Offering advice on personal development, relationships, and life decisions\n",
    "- Explaining how things work in everyday life and natural phenomena\n",
    "- Helping with writing, grammar, language learning, and communication\n",
    "- Providing travel information, recommendations, and cultural insights\n",
    "- Discussing business, economics, politics, and world affairs\n",
    "- Offering creative ideas for hobbies, crafts, and recreational activities\n",
    "- Explaining health, wellness, nutrition, and lifestyle topics\n",
    "- Helping with educational support across various non-technical subjects\n",
    "- Engaging in casual conversation and answering general curiosity questions\n",
    "- Providing summaries and explanations of complex topics in simple terms\n",
    "- Offering perspectives on ethical dilemmas and philosophical questions\"\"\"\n",
    "\n",
    "coding_emb = embed(CODING_AGENT_PROMPT, embedder)\n",
    "chat_emb = embed(CHAT_AGENT_PROMPT, embedder)\n",
    "\n",
    "# Evaluate routing accuracy\n",
    "evaluate_routing_accuracy(\"./data/prompts_2000.jsonl\", embedder, coding_emb, chat_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb0d17-41be-44d1-882e-230cfce68c8f",
   "metadata": {},
   "source": [
    "### That's another big improvement. Lets stop here for now and explore other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(embedder)\n",
    "gc.collect()                  # free unreferenced Python objects\n",
    "torch.cuda.empty_cache()      # return cached blocks to the driver\n",
    "torch.cuda.ipc_collect()      # (optional) collect interprocess cached memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d34169-1817-46fa-9c40-0d037aefbdc6",
   "metadata": {},
   "source": [
    "# Part Two:\n",
    "In part two we let a LLM calculate the property by classifying the prompt. We use a cheap LLM model (i.e. Llama 3.1 8B), and to query it we use Groq. I like Groq due the speed some of their servings offer.  \n",
    "We provide a detailed prompt to the LLM to identify if the incoming query is of class coding or class factual (not a coding related query).\n",
    "Now this approach may prove to be a slower one but for the current case this could be a reliable one. We also don't need to train any model when using this approach. However it may increase cost when the input query size is scaled whereas in the previous approach cost could be lowered by setting max_tokens (because in most cases (not all) part of the query is enough to classify)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5bad9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "363164c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = str  # \"coding\" | \"factual\"\n",
    "\n",
    "CLASSIFICATION_PROMPT = \"\"\"You are a routing classifier. Your job is to classify user queries into exactly one of two categories:\n",
    "\n",
    "1. \"coding\" - Programming, software development, technical implementation questions including:\n",
    "   - Writing, debugging, or optimizing code\n",
    "   - Programming languages (Python, JavaScript, Java, C++, etc.)\n",
    "   - Software development concepts (algorithms, data structures, OOP)\n",
    "   - Web development, APIs, databases\n",
    "   - DevOps, CI/CD, deployment\n",
    "   - Code reviews, testing, debugging\n",
    "   - Technical problem solving\n",
    "   - Programming frameworks and libraries\n",
    "\n",
    "2. \"factual\" - General knowledge, educational, conversational questions including:\n",
    "   - History, science, geography, current events\n",
    "   - Academic subjects (physics, chemistry, biology, math)\n",
    "   - Culture, arts, literature, entertainment\n",
    "   - Personal advice, relationships, life decisions\n",
    "   - General explanations of how things work\n",
    "   - Travel, recommendations, lifestyle\n",
    "   - Creative ideas, hobbies, crafts\n",
    "   - Health, wellness, nutrition\n",
    "   - Philosophy, ethics, social topics\n",
    "\n",
    "Analyze the following user query and respond with ONLY the category name: either \"coding\" or \"factual\"\n",
    "\n",
    "User query: {query}\n",
    "\n",
    "Category:\"\"\"\n",
    "\n",
    "def classify_with_llm(\n",
    "    prompt: str,\n",
    "    model: str = \"llama-3.1-8b-instant\",# Cheap model to try out\n",
    "    retries: int = 2,\n",
    "    backoff: float = 0.5,               # seconds; grows exponentially\n",
    "    on_error: Label = \"coding\",         # Expensive default to not let product impression suffer\n",
    "    fallback: Optional[Callable[[str], Label]] = None,\n",
    ") -> Label:\n",
    "    \"\"\"\n",
    "    Classify using Groq, returning exactly 'coding' or 'factual'.\n",
    "    Retries transient errors; optionally calls `fallback(prompt)` on failure.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        # If even the key is missing, go straight to fallback/default.\n",
    "        return fallback(prompt) if fallback else on_error\n",
    "\n",
    "    client = Groq(api_key=api_key)\n",
    "\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=model,\n",
    "                temperature=0,\n",
    "                max_tokens=5,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Reply with exactly one word: coding or factual.\"},\n",
    "                    {\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(query=prompt)},\n",
    "                ],\n",
    "            )\n",
    "            text = (resp.choices[0].message.content or \"\").strip().lower()\n",
    "            if \"coding\" in text:\n",
    "                return \"coding\"\n",
    "            if \"factual\" in text:\n",
    "                return \"factual\"\n",
    "            # Unexpected content: force a safe default\n",
    "            return on_error\n",
    "        except Exception as e:\n",
    "            # Optional: log e to your logger/Sentry here\n",
    "            if attempt < retries:\n",
    "                time.sleep(backoff * (2 ** attempt))\n",
    "            else:\n",
    "                # Exhausted retries → use fallback or default\n",
    "                return fallback(prompt) if fallback else on_error\n",
    "\n",
    "# Router node - uses LLM to determine category\n",
    "def llm_router_node(state: GraphState) -> GraphState:\n",
    "    prompt = state[\"prompt\"]\n",
    "    \n",
    "    # Use LLM to classify the prompt\n",
    "    predicted_category = classify_with_llm(prompt)\n",
    "    \n",
    "    state[\"agent_type\"] = \"coding\" if predicted_category == \"coding\" else \"chat\"\n",
    "    state[\"predicted_category\"] = predicted_category\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Create the LLM-based routing graph\n",
    "def create_llm_routing_graph():\n",
    "    workflow = StateGraph(GraphState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"llm_router\", llm_router_node)\n",
    "    workflow.add_node(\"coding_agent\", coding_agent_node)\n",
    "    workflow.add_node(\"chat_agent\", chat_agent_node)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"llm_router\")\n",
    "    \n",
    "    # Add conditional edges\n",
    "    workflow.add_conditional_edges(\n",
    "        \"llm_router\",\n",
    "        route_decision,\n",
    "        {\n",
    "            \"coding_agent\": \"coding_agent\",\n",
    "            \"chat_agent\": \"chat_agent\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add edges to end\n",
    "    workflow.add_edge(\"coding_agent\", END)\n",
    "    workflow.add_edge(\"chat_agent\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "def evaluate_llm_routing_accuracy(dataset_file: str):\n",
    "    \"\"\"Evaluate the LLM-based routing accuracy.\"\"\"\n",
    "    \n",
    "    # Load dataset\n",
    "    data = None\n",
    "    if Path(dataset_file).exists():\n",
    "        print(f\"Loading dataset from {dataset_file}...\")\n",
    "        data = load_dataset(dataset_file)\n",
    "    \n",
    "    if data is None:\n",
    "        print(\"Error: No dataset found. Please run dataset_create.py first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loaded {len(data)} prompts from dataset\")\n",
    "    \n",
    "    # Create the LLM routing graph\n",
    "    app = create_llm_routing_graph()\n",
    "    \n",
    "    # Statistics tracking\n",
    "    stats = defaultdict(int)\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Detailed results\n",
    "    results = []\n",
    "    \n",
    "    print(\"Evaluating LLM-based routing decisions...\")\n",
    "    print(\"Note: This may take a while depending on LLM response times...\")\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        if i % 50 == 0:  # Less frequent updates due to slower LLM calls\n",
    "            print(f\"Processed {i}/{len(data)} prompts...\")\n",
    "        \n",
    "        prompt = item['prompt']\n",
    "        actual_category = item['category']\n",
    "        \n",
    "        # Initial state\n",
    "        initial_state = {\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": \"\",\n",
    "            \"agent_type\": \"\",\n",
    "            \"predicted_category\": \"\",\n",
    "            \"actual_category\": actual_category\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Run the graph\n",
    "            result = app.invoke(initial_state)\n",
    "            predicted_category = result['predicted_category']\n",
    "            \n",
    "            # Count statistics\n",
    "            stats[f\"actual_{actual_category}\"] += 1\n",
    "            stats[f\"predicted_{predicted_category}\"] += 1\n",
    "            \n",
    "            # Check accuracy\n",
    "            is_correct = predicted_category == actual_category\n",
    "            if is_correct:\n",
    "                correct_predictions += 1\n",
    "                stats[f\"correct_{actual_category}\"] += 1\n",
    "            else:\n",
    "                stats[f\"incorrect_{actual_category}\"] += 1\n",
    "            \n",
    "            total_predictions += 1\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                'id': item.get('id', f'item_{i}'),\n",
    "                'prompt': prompt[:100] + '...' if len(prompt) > 100 else prompt,\n",
    "                'actual': actual_category,\n",
    "                'predicted': predicted_category,\n",
    "                'correct': is_correct,\n",
    "                'agent_routed_to': result['agent_type']\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item {i}: {e}\")\n",
    "            # Continue with next item\n",
    "            continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    overall_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    \n",
    "    coding_correct = stats['correct_coding']\n",
    "    coding_total = stats['actual_coding']\n",
    "    coding_accuracy = coding_correct / coding_total if coding_total > 0 else 0\n",
    "    \n",
    "    factual_correct = stats['correct_factual']\n",
    "    factual_total = stats['actual_factual']\n",
    "    factual_accuracy = factual_correct / factual_total if factual_total > 0 else 0\n",
    "    \n",
    "    # Confusion matrix\n",
    "    coding_predicted_as_coding = sum(1 for r in results if r['actual'] == 'coding' and r['predicted'] == 'coding')\n",
    "    coding_predicted_as_factual = sum(1 for r in results if r['actual'] == 'coding' and r['predicted'] == 'factual')\n",
    "    factual_predicted_as_coding = sum(1 for r in results if r['actual'] == 'factual' and r['predicted'] == 'coding')\n",
    "    factual_predicted_as_factual = sum(1 for r in results if r['actual'] == 'factual' and r['predicted'] == 'factual')\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LLM-BASED ROUTING ACCURACY EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nOverall Accuracy: {overall_accuracy:.3f} ({correct_predictions}/{total_predictions})\")\n",
    "    \n",
    "    print(f\"\\nPer-Category Accuracy:\")\n",
    "    print(f\"  Coding: {coding_accuracy:.3f} ({coding_correct}/{coding_total})\")\n",
    "    print(f\"  Factual: {factual_accuracy:.3f} ({factual_correct}/{factual_total})\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                    Predicted\")\n",
    "    print(f\"Actual      Coding    Factual\")\n",
    "    print(f\"Coding      {coding_predicted_as_coding:6d}    {coding_predicted_as_factual:7d}\")\n",
    "    print(f\"Factual     {factual_predicted_as_coding:6d}    {factual_predicted_as_factual:7d}\")\n",
    "    \n",
    "    print(f\"\\nRouting Statistics:\")\n",
    "    print(f\"  Total prompts routed to coding agent: {stats['predicted_coding']}\")\n",
    "    print(f\"  Total prompts routed to chat agent: {stats['predicted_factual']}\")\n",
    "    \n",
    "    # Show misclassifications\n",
    "    print(f\"\\nSample Misclassifications (first 5):\")\n",
    "    misclassified = [r for r in results if not r['correct']][:5]\n",
    "    for i, r in enumerate(misclassified, 1):\n",
    "        print(f\"  {i}. Actual: {r['actual']}, Predicted: {r['predicted']}\")\n",
    "        print(f\"     Prompt: {r['prompt']}\")\n",
    "        print()\n",
    "    \n",
    "    # Save results\n",
    "    output_file = \"llm_routing_evaluation_results.csv\"\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        fieldnames = ['id', 'prompt', 'actual', 'predicted', 'correct', 'agent_routed_to']\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    print(f\"Detailed results saved to: {output_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d694fff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ./data/prompts_2000.jsonl...\n",
      "Loaded 2000 prompts from dataset\n",
      "Evaluating LLM-based routing decisions...\n",
      "Note: This may take a while depending on LLM response times...\n",
      "Processed 0/2000 prompts...\n",
      "Processed 50/2000 prompts...\n",
      "Processed 50/2000 prompts...\n",
      "Processed 100/2000 prompts...\n",
      "Processed 100/2000 prompts...\n",
      "Processed 150/2000 prompts...\n",
      "Processed 150/2000 prompts...\n",
      "Processed 200/2000 prompts...\n",
      "Processed 200/2000 prompts...\n",
      "Processed 250/2000 prompts...\n",
      "Processed 250/2000 prompts...\n",
      "Processed 300/2000 prompts...\n",
      "Processed 300/2000 prompts...\n",
      "Processed 350/2000 prompts...\n",
      "Processed 350/2000 prompts...\n",
      "Processed 400/2000 prompts...\n",
      "Processed 400/2000 prompts...\n",
      "Processed 450/2000 prompts...\n",
      "Processed 450/2000 prompts...\n",
      "Processed 500/2000 prompts...\n",
      "Processed 500/2000 prompts...\n",
      "Processed 550/2000 prompts...\n",
      "Processed 550/2000 prompts...\n",
      "Processed 600/2000 prompts...\n",
      "Processed 600/2000 prompts...\n",
      "Processed 650/2000 prompts...\n",
      "Processed 650/2000 prompts...\n",
      "Processed 700/2000 prompts...\n",
      "Processed 700/2000 prompts...\n",
      "Processed 750/2000 prompts...\n",
      "Processed 750/2000 prompts...\n",
      "Processed 800/2000 prompts...\n",
      "Processed 800/2000 prompts...\n",
      "Processed 850/2000 prompts...\n",
      "Processed 850/2000 prompts...\n",
      "Processed 900/2000 prompts...\n",
      "Processed 900/2000 prompts...\n",
      "Processed 950/2000 prompts...\n",
      "Processed 950/2000 prompts...\n",
      "Processed 1000/2000 prompts...\n",
      "Processed 1000/2000 prompts...\n",
      "Processed 1050/2000 prompts...\n",
      "Processed 1050/2000 prompts...\n",
      "Processed 1100/2000 prompts...\n",
      "Processed 1100/2000 prompts...\n",
      "Processed 1150/2000 prompts...\n",
      "Processed 1150/2000 prompts...\n",
      "Processed 1200/2000 prompts...\n",
      "Processed 1200/2000 prompts...\n",
      "Processed 1250/2000 prompts...\n",
      "Processed 1250/2000 prompts...\n",
      "Processed 1300/2000 prompts...\n",
      "Processed 1300/2000 prompts...\n",
      "Processed 1350/2000 prompts...\n",
      "Processed 1350/2000 prompts...\n",
      "Processed 1400/2000 prompts...\n",
      "Processed 1400/2000 prompts...\n",
      "Processed 1450/2000 prompts...\n",
      "Processed 1450/2000 prompts...\n",
      "Processed 1500/2000 prompts...\n",
      "Processed 1500/2000 prompts...\n",
      "Processed 1550/2000 prompts...\n",
      "Processed 1550/2000 prompts...\n",
      "Processed 1600/2000 prompts...\n",
      "Processed 1600/2000 prompts...\n",
      "Processed 1650/2000 prompts...\n",
      "Processed 1650/2000 prompts...\n",
      "Processed 1700/2000 prompts...\n",
      "Processed 1700/2000 prompts...\n",
      "Processed 1750/2000 prompts...\n",
      "Processed 1750/2000 prompts...\n",
      "Processed 1800/2000 prompts...\n",
      "Processed 1800/2000 prompts...\n",
      "Processed 1850/2000 prompts...\n",
      "Processed 1850/2000 prompts...\n",
      "Processed 1900/2000 prompts...\n",
      "Processed 1900/2000 prompts...\n",
      "Processed 1950/2000 prompts...\n",
      "Processed 1950/2000 prompts...\n",
      "\n",
      "============================================================\n",
      "LLM-BASED ROUTING ACCURACY EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.974 (1947/2000)\n",
      "\n",
      "Per-Category Accuracy:\n",
      "  Coding: 0.995 (995/1000)\n",
      "  Factual: 0.952 (952/1000)\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted\n",
      "Actual      Coding    Factual\n",
      "Coding         995          5\n",
      "Factual         48        952\n",
      "\n",
      "Routing Statistics:\n",
      "  Total prompts routed to coding agent: 1043\n",
      "  Total prompts routed to chat agent: 957\n",
      "\n",
      "Sample Misclassifications (first 5):\n",
      "  1. Actual: factual, Predicted: coding\n",
      "     Prompt: What is the main difference between ORC and Parquet?\n",
      "\n",
      "  2. Actual: factual, Predicted: coding\n",
      "     Prompt: What is Apache Hive?\n",
      "\n",
      "  3. Actual: factual, Predicted: coding\n",
      "     Prompt: What is the Cassandra database?\n",
      "\n",
      "  4. Actual: coding, Predicted: factual\n",
      "     Prompt: Classify the given sentence as short or long.\n",
      "\n",
      "  5. Actual: factual, Predicted: coding\n",
      "     Prompt: What is the value of x if x^2 = 4?\n",
      "\n",
      "Detailed results saved to: llm_routing_evaluation_results.csv\n",
      "\n",
      "============================================================\n",
      "LLM-BASED ROUTING ACCURACY EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Overall Accuracy: 0.974 (1947/2000)\n",
      "\n",
      "Per-Category Accuracy:\n",
      "  Coding: 0.995 (995/1000)\n",
      "  Factual: 0.952 (952/1000)\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted\n",
      "Actual      Coding    Factual\n",
      "Coding         995          5\n",
      "Factual         48        952\n",
      "\n",
      "Routing Statistics:\n",
      "  Total prompts routed to coding agent: 1043\n",
      "  Total prompts routed to chat agent: 957\n",
      "\n",
      "Sample Misclassifications (first 5):\n",
      "  1. Actual: factual, Predicted: coding\n",
      "     Prompt: What is the main difference between ORC and Parquet?\n",
      "\n",
      "  2. Actual: factual, Predicted: coding\n",
      "     Prompt: What is Apache Hive?\n",
      "\n",
      "  3. Actual: factual, Predicted: coding\n",
      "     Prompt: What is the Cassandra database?\n",
      "\n",
      "  4. Actual: coding, Predicted: factual\n",
      "     Prompt: Classify the given sentence as short or long.\n",
      "\n",
      "  5. Actual: factual, Predicted: coding\n",
      "     Prompt: What is the value of x if x^2 = 4?\n",
      "\n",
      "Detailed results saved to: llm_routing_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "evaluate_llm_routing_accuracy(\"./data/prompts_2000.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4492b618",
   "metadata": {},
   "source": [
    "Alright so our priliminary experiments show that categorical classification is better when the model is a LLM and not an embedding model. But again, depending on what we're trying optimise, either of the approach can be more fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6edaab6-5ff8-47cc-a6c2-a9fa6d50b3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
