{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b843be",
   "metadata": {},
   "source": [
    "# Dataset Creation for Router Training (For Part One and Two)\n",
    "\n",
    "For the same of demonstration for the current task at hand, here we build a balanced dataset for training semantic routers. The dataset consists of:\n",
    "- **Factual prompts**: General knowledge questions from Dolly-15k dataset\n",
    "- **Coding prompts**: Programming-related questions from CodeAlpaca-20k dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a8d8fb-4fc8-40b1-9703-4041f2eb606b",
   "metadata": {},
   "source": [
    "The dataset then finally has 2000 queries, 1000 for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf6c9f93-e8df-4748-9c12-868adc4c6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6266bef5-4e1f-4407-9956-aab37882c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "SEED = 999999\n",
    "TOTAL = 2000\n",
    "FACTUAL_TARGET = TOTAL // 2\n",
    "CODING_TARGET = TOTAL - FACTUAL_TARGET\n",
    "\n",
    "FACTUAL_SOURCE = \"databricks/databricks-dolly-15k\"\n",
    "CODING_SOURCE = \"sahil2801/CodeAlpaca-20k\"\n",
    "\n",
    "# Skip first N samples to get fresh data for testing\n",
    "SKIP_FIRST = False  # Set to False to disable skipping\n",
    "SKIP_COUNT = 5000  # Number of samples to skip from the beginning\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc4ca0af-fcac-4f48-9586-7346a84d7604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristics for filtering\n",
    "CODING_KEYWORDS = {\n",
    "    \"python\",\"java\",\"javascript\",\"js\",\"typescript\",\"ts\",\"c++\",\"c#\",\"golang\",\"go\",\"rust\",\"ruby\",\"php\",\"sql\",\n",
    "    \"bash\",\"shell\",\"regex\",\"html\",\"css\",\"json\",\"yaml\",\"xml\",\"api\",\"sdk\",\"library\",\"framework\",\"tensorflow\",\n",
    "    \"pytorch\",\"numpy\",\"pandas\",\"sklearn\",\"django\",\"flask\",\"fastapi\",\"node\",\"react\",\"vue\",\"svelte\",\"angular\",\n",
    "    \"kotlin\",\"swift\",\"scala\",\"haskell\",\"matlab\",\"octave\",\"r \",\"julia\",\"notebook\",\"jupyter\",\"colab\",\n",
    "    \"function\",\"class\",\"method\",\"variable\",\"loop\",\"algorithm\",\"complexity\",\"big o\",\"o(n)\",\"runtime\",\n",
    "    \"compile\",\"build\",\"debug\",\"error\",\"stack trace\",\"exception\",\"unit test\",\"test case\",\"package\",\n",
    "    \"import\",\"module\",\"script\",\"snippet\",\"code\",\"coding\",\"program\",\"programming\"\n",
    "}\n",
    "# words that often indicate general factual/trivia/expository questions\n",
    "FACTUAL_HINTS = {\n",
    "    \"who\",\"what\",\"when\",\"where\",\"why\",\"how\",\"which\",\"name\",\"define\",\"explain\",\"describe\",\"compare\",\"contrast\",\n",
    "    \"list\",\"identify\",\"summarize\",\"outline\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c2899-b1e3-4e17-85da-cb190da783f2",
   "metadata": {},
   "source": [
    "### Utility functions\n",
    "They are almost unnecesasry but for the sake of demonstration of methods and not data, why not take clean data and focus on model comparison. We'll see that even filter the most obvious and generic examples our baseline doesn't reach a 100% at start, this will still need tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "236ab676-418b-46a0-b1b1-9cfcabcc7b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_coding_text(text: str) -> bool:\n",
    "    t = text.lower()\n",
    "    if \"```\" in t:  # code fences\n",
    "        return True\n",
    "    # keyword hit\n",
    "    return any(kw in t for kw in CODING_KEYWORDS)\n",
    "\n",
    "\n",
    "def is_factual_text(text: str) -> bool:\n",
    "    t = text.strip().lower()\n",
    "    # generally a question or expository ask, without obvious coding markers\n",
    "    if is_coding_text(t):\n",
    "        return False\n",
    "    # question-y or factual vibe\n",
    "    if \"?\" in t or any(t.startswith(h) for h in FACTUAL_HINTS):\n",
    "        return True\n",
    "    # statements that ask to explain/define/etc.\n",
    "    return any(h in t.split()[:3] for h in [\"explain\", \"define\", \"describe\", \"summarize\"])\n",
    "\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    # Light cleanup\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a9d7800-71e0-401e-b032-5f54138201fa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sample_dolly_factual(n_target: int):\n",
    "    \"\"\"From Dolly 15k, pick Q&A‑ish factual prompts, exclude coding.\"\"\"\n",
    "    ds = load_dataset(FACTUAL_SOURCE, split=\"train\")\n",
    "    records = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for i, ex in enumerate(ds):\n",
    "        # Skip first N samples if SKIP_FIRST is enabled\n",
    "        if SKIP_FIRST and skipped_count < SKIP_COUNT:\n",
    "            instr = ex.get(\"instruction\") or \"\"\n",
    "            ctx = ex.get(\"context\") or \"\"\n",
    "            text_for_filter = \" \".join([instr, ctx]).strip()\n",
    "            if text_for_filter:  # Only count valid samples towards skip count\n",
    "                category = (ex.get(\"category\") or \"\").lower()\n",
    "                likely_factual_cat = category in {\"open_qa\", \"closed_qa\", \"information_extraction\", \"classification\"}\n",
    "                if likely_factual_cat and is_factual_text(instr or ctx):\n",
    "                    if not is_coding_text(instr) and not is_coding_text(ctx):\n",
    "                        skipped_count += 1\n",
    "            continue\n",
    "        instr = ex.get(\"instruction\") or \"\"\n",
    "        ctx = ex.get(\"context\") or \"\"\n",
    "        resp = ex.get(\"response\") or \"\"\n",
    "\n",
    "        text_for_filter = \" \".join([instr, ctx]).strip()\n",
    "        if not text_for_filter:\n",
    "            continue\n",
    "\n",
    "        # Prefer Dolly's open_qa & classification/extraction that look factual\n",
    "        category = (ex.get(\"category\") or \"\").lower()\n",
    "        likely_factual_cat = category in {\"open_qa\", \"closed_qa\", \"information_extraction\", \"classification\"}\n",
    "\n",
    "        if likely_factual_cat and is_factual_text(instr or ctx):\n",
    "            if not is_coding_text(instr) and not is_coding_text(ctx):\n",
    "                prompt = clean_text(instr if instr else ctx)\n",
    "                if len(prompt) < 15:  # avoid overly short\n",
    "                    continue\n",
    "                records.append({\n",
    "                    \"id\": f\"dolly-{i}\",\n",
    "                    \"category\": \"factual\",\n",
    "                    \"prompt\": prompt,\n",
    "                    \"response\": clean_text(resp) if resp else \"\",\n",
    "                    \"source\": FACTUAL_SOURCE,\n",
    "                    \"source_id\": i,\n",
    "                })\n",
    "\n",
    "    # If we’re short, relax the category constraint but keep non-coding & factual vibe\n",
    "    if len(records) < n_target:\n",
    "        for i, ex in enumerate(ds):\n",
    "            if len(records) >= n_target * 2:  # cap oversampling before sampling down\n",
    "                break\n",
    "            instr = ex.get(\"instruction\") or \"\"\n",
    "            ctx = ex.get(\"context\") or \"\"\n",
    "            resp = ex.get(\"response\") or \"\"\n",
    "            text_for_filter = \" \".join([instr, ctx]).strip()\n",
    "            if not text_for_filter:\n",
    "                continue\n",
    "            if is_factual_text(instr or ctx) and not is_coding_text(instr) and not is_coding_text(ctx):\n",
    "                prompt = clean_text(instr if instr else ctx)\n",
    "                if len(prompt) < 15:\n",
    "                    continue\n",
    "                records.append({\n",
    "                    \"id\": f\"dolly-{i}\",\n",
    "                    \"category\": \"factual\",\n",
    "                    \"prompt\": prompt,\n",
    "                    \"response\": clean_text(resp) if resp else \"\",\n",
    "                    \"source\": FACTUAL_SOURCE,\n",
    "                    \"source_id\": i,\n",
    "                })\n",
    "\n",
    "    if SKIP_FIRST:\n",
    "        print(f\"Dolly factual: Skipped first {skipped_count} valid samples\")\n",
    "    \n",
    "    random.shuffle(records)\n",
    "    return records[:n_target]\n",
    "\n",
    "\n",
    "def sample_codealpaca_coding(n_target: int):\n",
    "    \"\"\"From CodeAlpaca-20k, sample coding prompts (most are coding by construction).\"\"\"\n",
    "    ds = load_dataset(CODING_SOURCE, split=\"train\")\n",
    "    candidates = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for i, ex in enumerate(ds):\n",
    "        # Skip first N samples if SKIP_FIRST is enabled\n",
    "        if SKIP_FIRST and skipped_count < SKIP_COUNT:\n",
    "            instr = ex.get(\"instruction\") or \"\"\n",
    "            if instr and is_coding_text(instr):  # Only count valid coding samples towards skip count\n",
    "                skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        instr = ex.get(\"instruction\") or \"\"\n",
    "        output = ex.get(\"output\") or ex.get(\"response\") or \"\"\n",
    "        if not instr:\n",
    "            continue\n",
    "        if is_coding_text(instr):\n",
    "            candidates.append({\n",
    "                \"id\": f\"codealpaca-{i}\",\n",
    "                \"category\": \"coding\",\n",
    "                \"prompt\": clean_text(instr),\n",
    "                \"response\": clean_text(output) if output else \"\",\n",
    "                \"source\": CODING_SOURCE,\n",
    "                \"source_id\": i,\n",
    "            })\n",
    "\n",
    "    # If not enough matched by keywords, backfill with any examples (CodeAlpaca is coding-oriented anyway)\n",
    "    if len(candidates) < n_target:\n",
    "        skipped_count_relaxed = 0\n",
    "        for i, ex in enumerate(ds):\n",
    "            if len(candidates) >= n_target * 2:\n",
    "                break\n",
    "                \n",
    "            # Skip first N samples if SKIP_FIRST is enabled (for relaxed pass)\n",
    "            if SKIP_FIRST and skipped_count_relaxed < SKIP_COUNT:\n",
    "                instr = ex.get(\"instruction\") or \"\"\n",
    "                if instr:  # Count any valid instruction towards skip count\n",
    "                    skipped_count_relaxed += 1\n",
    "                continue\n",
    "            instr = ex.get(\"instruction\") or \"\"\n",
    "            output = ex.get(\"output\") or ex.get(\"response\") or \"\"\n",
    "            if not instr:\n",
    "                continue\n",
    "            candidates.append({\n",
    "                \"id\": f\"codealpaca-{i}\",\n",
    "                \"category\": \"coding\",\n",
    "                \"prompt\": clean_text(instr),\n",
    "                \"response\": clean_text(output) if output else \"\",\n",
    "                \"source\": CODING_SOURCE,\n",
    "                \"source_id\": i,\n",
    "            })\n",
    "\n",
    "    if SKIP_FIRST:\n",
    "        print(f\"CodeAlpaca coding: Skipped first {skipped_count} valid samples\")\n",
    "    \n",
    "    random.shuffle(candidates)\n",
    "    return candidates[:n_target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44e5b0f9-faf4-4f83-87fb-fc0a79d4d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    print(\"Loading & sampling…\")\n",
    "    if SKIP_FIRST:\n",
    "        print(f\"SKIP_FIRST enabled: Skipping first {SKIP_COUNT} valid samples from each source\")\n",
    "    else:\n",
    "        print(\"SKIP_FIRST disabled: Using samples from the beginning\")\n",
    "    \n",
    "    factual = sample_dolly_factual(FACTUAL_TARGET)\n",
    "    print(f\"Factual collected: {len(factual)}\")\n",
    "\n",
    "    coding = sample_codealpaca_coding(CODING_TARGET)\n",
    "    print(f\"Coding collected: {len(coding)}\")\n",
    "\n",
    "    combined = factual + coding\n",
    "    random.shuffle(combined)\n",
    "\n",
    "    # Update output filenames to indicate if skip was used\n",
    "    if SKIP_FIRST:\n",
    "        out_jsonl = Path(f\"prompts_{TOTAL}_skip{SKIP_COUNT}.jsonl\")\n",
    "        out_csv = Path(f\"prompts_{TOTAL}_skip{SKIP_COUNT}.csv\")\n",
    "    else:\n",
    "        out_jsonl = Path(f\"prompts_{TOTAL}.jsonl\")\n",
    "        out_csv = Path(f\"prompts_{TOTAL}.csv\")\n",
    "\n",
    "    print(f\"Writing {out_jsonl} …\")\n",
    "    with out_jsonl.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for row in combined:\n",
    "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Writing {out_csv} …\")\n",
    "    with out_csv.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"id\",\"category\",\"prompt\",\"response\",\"source\",\"source_id\"])\n",
    "        w.writeheader()\n",
    "        for row in combined:\n",
    "            w.writerow(row)\n",
    "\n",
    "    # Simple stats\n",
    "    def avg_len(key):\n",
    "        xs = [len(r[key].split()) for r in combined if r[key]]\n",
    "        return round(sum(xs)/len(xs), 1) if xs else 0.0\n",
    "\n",
    "    print(\"Done!\")\n",
    "    print(f\"Total: {len(combined)} (factual={len(factual)}, coding={len(coding)})\")\n",
    "    if SKIP_FIRST:\n",
    "        print(f\"Dataset created with skip_first={SKIP_COUNT} - contains fresh data for testing\")\n",
    "    print(f\"Avg prompt length (words): {avg_len('prompt')}\")\n",
    "    print(f\"Avg response length (words): {avg_len('response')}\")\n",
    "    print(f\"Output files: {out_jsonl.name}, {out_csv.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81a76bf1-c54e-4cee-9c3d-433987be6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading & sampling…\n",
      "SKIP_FIRST disabled: Using samples from the beginning\n",
      "Factual collected: 1000\n",
      "Coding collected: 1000\n",
      "Writing prompts_2000.jsonl …\n",
      "Writing prompts_2000.csv …\n",
      "Done!\n",
      "Total: 2000 (factual=1000, coding=1000)\n",
      "Avg prompt length (words): 10.9\n",
      "Avg response length (words): 38.9\n",
      "Output files: prompts_2000.jsonl, prompts_2000.csv\n"
     ]
    }
   ],
   "source": [
    "create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9510ccb5-7df5-483b-a22b-d9bd4bb20ced",
   "metadata": {},
   "source": [
    "# Dataset Creation for Router Training (For Part Three)\n",
    "\n",
    "To train a BERT-like model for routing we need prompts and labels which answer if the prompt should be routed to a weak model or a strong model.\n",
    "We start with taking all 2000 of previously generated prompts and calling Llama 3.1 8B (our weak model) to generate response.\n",
    "To generate we use OpenRouter.ai becuase it is cheap, some API calls might fail but few retries are enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae37c651-1537-43a0-be28-b25de948efe1",
   "metadata": {},
   "source": [
    "To do so we use the following script.   \n",
    "(I am sorry for not making doing it in the notebook, most of my work was done in scripts because I am comfortable with that, but I understand if the Nexos team prefers only notebooks I can totally get comfortable with that at the job)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ec4bc-6384-4e11-9c38-ec4712170643",
   "metadata": {},
   "source": [
    "Note: this part of the notebook requires an API key for openrouter.ai, you can place it in a .env file and it should work. See `.env.example`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd898e28-c9dd-402a-a79f-a3c9c91f09cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: generate_responses.py [-h] [--input INPUT] [--model MODEL]\n",
      "                             [--output OUTPUT] [--max-tokens MAX_TOKENS]\n",
      "                             [--temperature TEMPERATURE]\n",
      "                             [--max-concurrent MAX_CONCURRENT] [--limit LIMIT]\n",
      "\n",
      "Generate responses for dataset using OpenRouter API (Async)\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --input INPUT, -i INPUT\n",
      "                        Input dataset file (default: prompts_2000.jsonl)\n",
      "  --model MODEL, -m MODEL\n",
      "                        Model to use for generation (default: meta-\n",
      "                        llama/llama-3.1-8b-instruct)\n",
      "  --output OUTPUT, -o OUTPUT\n",
      "                        Output file prefix (default: auto-generated based on\n",
      "                        model and timestamp)\n",
      "  --max-tokens MAX_TOKENS\n",
      "                        Maximum tokens to generate (default: 1000)\n",
      "  --temperature TEMPERATURE\n",
      "                        Temperature for generation (default: 0.7)\n",
      "  --max-concurrent MAX_CONCURRENT\n",
      "                        Maximum concurrent requests (default: 10, increase for\n",
      "                        faster processing)\n",
      "  --limit LIMIT         Limit processing to first N prompts (for testing)\n"
     ]
    }
   ],
   "source": [
    "!python generate_responses.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ec013-fea1-42a5-9e19-373fa81bed34",
   "metadata": {},
   "source": [
    "The command produces a dataset with response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046e9942-0c5a-461c-a3ce-e56129450d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_responses.py --max-concurrent 5 --input prompts_2000.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a879699-612a-4eab-8553-6893baa8a347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5fd61a7-0ad6-4d08-825f-59846ca51d4f",
   "metadata": {},
   "source": [
    "Once we have our responses generated, we need to run them through a stronger model and generate ratings.  \n",
    "To generate ratings, we present the prompt and the response from our weaker model to a stronger model, and ask it to produce a rating on a scale of 1 to 5.  \n",
    "For the strong model we use `openai/gpt-oss-120b` as it is substantially better than our weak model Llama 3.1 8B in majority of the benchmarks and also does well on SWE-bench. We make the assumption that it would be a good judge.\n",
    "\n",
    "\n",
    "The following is the prompt template we use:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3994aecf-5484-4bc2-8503-cfac115b88d8",
   "metadata": {},
   "source": [
    "```\n",
    "\"\"\"You are an expert evaluator tasked with rating the quality of AI model responses. \n",
    "\n",
    "Please rate the following response on a scale from 1 to 5 based on these criteria:\n",
    "- **Accuracy**: Is the information correct?\n",
    "- **Relevance**: Does it directly address the user's question/request?\n",
    "- **Completeness**: Does it provide a thorough answer?\n",
    "- **Clarity**: Is it well-written and easy to understand?\n",
    "- **Helpfulness**: Would this response be useful to the user?\n",
    "\n",
    "Rating Scale:\n",
    "- **1**: Very Poor - Incorrect, irrelevant, or unhelpful\n",
    "- **2**: Poor - Mostly incorrect or not very helpful\n",
    "- **3**: Average - Somewhat helpful but has issues\n",
    "- **4**: Good - Helpful and mostly accurate\n",
    "- **5**: Excellent - Highly accurate, relevant, and helpful\n",
    "\n",
    "**Original User Prompt:**\n",
    "{original_prompt}\n",
    "\n",
    "**Model Response to Rate:**\n",
    "{model_response}\n",
    "\n",
    "Please provide only a single number (1, 2, 3, 4, or 5) as your rating, followed by a brief explanation in parentheses.\n",
    "\n",
    "Rating:\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7bd74b-d61e-4d28-a580-3e2d44c41720",
   "metadata": {},
   "source": [
    "To generate response, the following is the interface of the script responsible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6fa8a34-5e68-4d5d-9422-ca05f87af342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: rate_responses.py [-h] [--input INPUT] [--rating-model RATING_MODEL]\n",
      "                         [--output OUTPUT] [--max-concurrent MAX_CONCURRENT]\n",
      "                         [--max-retries MAX_RETRIES] [--limit LIMIT]\n",
      "\n",
      "Rate responses in enhanced dataset using OpenRouter API (Async)\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --input INPUT, -i INPUT\n",
      "                        Input enhanced dataset file\n",
      "  --rating-model RATING_MODEL, -r RATING_MODEL\n",
      "                        Model to use for rating (default: openai/gpt-oss-120b)\n",
      "  --output OUTPUT, -o OUTPUT\n",
      "                        Output file prefix (default: auto-generated based on\n",
      "                        input and timestamp)\n",
      "  --max-concurrent MAX_CONCURRENT\n",
      "                        Maximum concurrent requests (default: 15)\n",
      "  --max-retries MAX_RETRIES\n",
      "                        Maximum retries per failed request (default: 3)\n",
      "  --limit LIMIT         Limit processing to first N items (for testing)\n"
     ]
    }
   ],
   "source": [
    "!python rate_responses.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d508db-cf3d-477c-b373-13ebcda4efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python rate_responses.py --input rated_enhanced_dataset_meta_llama_llama_3.1_8b_instruct_20250811_140130_20250811_151904.jsonl --max-retries 5 max-concurrent 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd805b84-b407-452b-a9c9-67c6401a3482",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b689bf-298e-4ef4-ae0e-3720b3c8c7de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
